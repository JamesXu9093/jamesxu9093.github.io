<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[Spark学习归纳总结]]></title>
      <url>https://jamesxu9093.github.io/2016/12/15/spark%E5%AD%A6%E4%B9%A0%E5%BD%92%E7%BA%B3/</url>
      <content type="html"><![CDATA[<p>spark作为当下大数据领域最炙手可热的数据处理框架，版本的更新、迭代速度也是非常之快，由学习的时候1.3的版本，到1.4，到1.6，到现在的2.0版本。<br>以下为本人学习过程以及需要努力的方向：<br>1、spark原理：rdd(新的2.0版本rdd并不是推荐的数据表示形式，而是DataFram)<br>2、spark-streaming<br>3、spark-sql<br>4、spark-mllib<br><a id="more"></a></p>
<h2 id="资源归纳"><a href="#资源归纳" class="headerlink" title="资源归纳"></a>资源归纳</h2><p>1、快速入门<br><a href="http://spark.apache.org/docs/latest/quick-start.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/quick-start.html</a><br>讲解map，reduce，flatmap，reducebykey等的用法<br>spark程序中可以应用任意的scala和java语言的特性(jar包，工具类，方法等，见例子)</p>
<p>2、官方手册<br><a href="http://spark.apache.org/docs/latest/programming-guide.html" target="_blank" rel="external">http://spark.apache.org/docs/latest/programming-guide.html</a></p>
<p>3、litaotao的归纳（spark科技日报）<br><a href="http://litaotao.github.io/introduction-to-spark?s=inner" target="_blank" rel="external">http://litaotao.github.io/introduction-to-spark?s=inner</a><br><a href="http://mp.weixin.qq.com/s?__biz=MzAwNzIzMDY5OA==&amp;mid=2651423985&amp;idx=1&amp;sn=962a12f58067618df12d20ab7a48868c&amp;scene=21#wechat_redirect" target="_blank" rel="external">http://mp.weixin.qq.com/s?__biz=MzAwNzIzMDY5OA==&amp;mid=2651423985&amp;idx=1&amp;sn=962a12f58067618df12d20ab7a48868c&amp;scene=21#wechat_redirect</a></p>
<h2 id="spark原理"><a href="#spark原理" class="headerlink" title="spark原理"></a>spark原理</h2><h3 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h3><p><strong>1.what’s spark？</strong><br>Apache Spark™ is a fast and general engine for large-scale data processing.</p>
<p><strong>2.Why is spark?</strong><br><em>Speed</em><br>Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x faster on disk.<br>Apache Spark has an advanced DAG execution engine that supports cyclic data flow and in-memory computing.</p>
<p><em>Ease of Use</em><br>Write applications quickly in Java, Scala, Python, R.<br>Spark offers over 80 high-level operators that make it easy to build parallel apps. And you can use it interactively from the Scala, Python and R shells.</p>
<p><em>Generality</em><br>Combine SQL, streaming, and complex analytics.<br>Spark powers a stack of libraries including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming. You can combine these libraries seamlessly in the same application.</p>
<p><em>Runs Everywhere</em><br>Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.<br>You can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN, or on Apache Mesos. Access data in HDFS, Cassandra, HBase, Hive, Tachyon, and any Hadoop data source.</p>
<p><em>One stack rule them all !!!</em></p>
<p><strong>3. 一栈式解决方案</strong><br>实时流式计算：Stream Processing<br>批处理：Batch Process<br>点对点查询：Ad-hoc Queries</p>
<p><strong>4.与hadoop对比</strong><br>hadoop为什么慢：额外的复制，序列化，磁盘IO开销<br>spark为什么快：内存计算，DAG优化</p>
<p><strong>5.spark运行模式</strong><br>Local(多用于测试)<br>Standalone<br>Mesos<br>Yarn</p>
<h3 id="内核分析"><a href="#内核分析" class="headerlink" title="内核分析"></a>内核分析</h3><p><strong>1.spark介绍</strong><br>Spark是起源于美国加州大学伯克利分校AMPLab的大数据计算平台，在2010年开源，目前是Apache软件基金会的顶级项目。随着Spark在大数据计算领域的暂露头角，越来越多的企业开始关注和使用。2014年11月，Spark在Daytona Gray Sort 100TB Benchmark竞赛中打破了由Hadoop MapReduce保持的排序记录。Spark利用1/10的节点数，把100TB数据的排序时间从72分钟提高到了23分钟[1]。</p>
<p>Spark在架构上包括内核部分和4个官方子模块–Spark SQL、Spark Streaming、机器学习库MLlib和图计算库GraphX。图1所示为Spark在伯克利的数据分析软件栈BDAS [2]（<a href="https://amplab.cs.berkeley.edu/software/" target="_blank" rel="external">Berkeley Data Analytics Stack</a>）中的位置。可见Spark专注于数据的计算，而数据的存储在生产环境中往往还是由Hadoop分布式文件系统HDFS承担。</p>
<p><img src="/blog-img/1219-1.png" alt=""></p>
<p>Spark被设计成支持多场景的通用大数据计算平台，它可以解决大数据计算中的批处理，交互查询及流式计算等核心问题。Spark可以从多数据源的读取数据，并且拥有不断发展的机器学习库和图计算库供开发者使用。数据和计算在Spark内核及Spark的子模块中是打通的，这就意味着Spark内核和子模块之间成为一个整体。Spark的各个子模块以Spark内核为基础，进一步支持更多的计算场景，例如使用Spark SQL读入的数据可以作为机器学习库MLlib的输入。表1列举了一些在Spark平台上的计算场景。<br><img src="/blog-img/1219-2.png" alt=""><br>在本文写作是，Spark的最新版本为1.2.0，文中的示例代码也来自于这个版本。</p>
<p><strong>2. Spark内核介绍</strong></p>
<p>相信大数据工程师都非常了解Hadoop MapReduce一个最大的问题是在很多应用场景中速度非常慢，只适合离线的计算任务。这是由于MapReduce需要将任务划分成map和reduce两个阶段，map阶段产生的中间结果要写回磁盘，而在这两个阶段之间需要进行shuffle操作。Shuffle操作需要从网络中的各个节点进行数据拷贝，使其往往成为最为耗时的步骤，这也是Hadoop MapReduce慢的根本原因之一，大量的时间耗费在网络磁盘IO中而不是用于计算。在一些特定的计算场景中，例如像逻辑回归这样的迭代式的计算，MapReduce的弊端会显得更加明显。<br>那Spark是如果设计分布式计算的呢？首先我们需要理解Spark中最重要的概念–弹性分布数据集（Resilient Distributed Dataset），也就是RDD。</p>
<p><strong>2.1 弹性分布数据集RDD</strong></p>
<p>RDD是Spark中对数据和计算的抽象，是Spark中最核心的概念，它表示已被分片（partition），不可变的并能够被并行操作的数据集合。对RDD的操作分为两种transformation和action。Transformation操作是通过转换从一个或多个RDD生成新的RDD。Action操作是从RDD生成最后的计算结果。在Spark最新的版本中，提供丰富的transformation和action操作，比起MapReduce计算模型中仅有的两种操作，会大大简化程序开发的难度。</p>
<p>RDD的生成方式只有两种，一是从数据源读入，另一种就是从其它RDD通过transformation操作转换。一个典型的Spark程序就是通过Spark上下文环境（SparkContext）生成一个或多个RDD，在这些RDD上通过一系列的transformation操作生成最终的RDD，最后通过调用最终RDD的action方法输出结果。</p>
<p>每个RDD都可以用下面5个特性来表示，其中后两个为可选的：<br><em>&gt;&gt;&gt;&gt;&gt;分片列表（数据块列表）</em><br><em>&gt;&gt;&gt;&gt;&gt;计算每个分片的函数</em><br><em>&gt;&gt;&gt;&gt;&gt;对父RDD的依赖列表</em><br><em>&gt;&gt;&gt;&gt;&gt;对key-value类型的RDD的分片器（Partitioner）（可选）</em><br><em>&gt;&gt;&gt;&gt;&gt;每个数据分片的预定义地址列表（如HDFS上的数据块的地址）（可选）</em></p>
<p>虽然Spark是基于内存的计算，但RDD不光可以存储在内存中，根据useDisk、useMemory、useOffHeap, deserialized、replication五个参数的组合Spark提供了12种存储级别，在后面介绍RDD的容错机制时，我们会进一步理解。值得注意的是当StorageLevel设置成OFF_HEAP时，RDD实际被保存到Tachyon[5]中。Tachyon是一个基于内存的分布式文件系统，目前正在快速发展，本文不做详细介绍，可以通过其官方网站进一步了解。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></div><div class="line">    private var _useDisk: <span class="type">Boolean</span>,</div><div class="line">    private var _useMemory: <span class="type">Boolean</span>,</div><div class="line">    private var _useOffHeap: <span class="type">Boolean</span>,</div><div class="line">    private var _deserialized: <span class="type">Boolean</span>,</div><div class="line">    private var _replication: <span class="type">Int</span> = 1)</div><div class="line">  <span class="keyword">extends</span> <span class="type">Externalizable</span> &#123; <span class="comment">//…</span></div><div class="line">&#125;</div><div class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</div><div class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</div><div class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</div><div class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</div><div class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</div><div class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</div><div class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</div><div class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</div><div class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</div><div class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</div><div class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</div><div class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>)</div></pre></td></tr></table></figure></p>
<p><strong>2.2 DAG、Stage与任务的生成</strong></p>
<p>Spark的计算发生在RDD的action操作，而对action之前的所有transformation，Spark只是记录下RDD生成的轨迹，而不会触发真正的计算。</p>
<p>Spark内核会在需要计算发生的时刻绘制一张关于计算路径的有向无环图，也就是DAG。举个例子，在图2中，从输入中逻辑上生成A和C两个RDD，经过一系列transformation操作，逻辑上生成了F，注意，我们说的是逻辑上，因为这时候计算没有发生，Spark内核做的事情只是记录了RDD的生成和依赖关系。当F要进行输出时，也就是F进行了action操作，Spark会根据RDD的依赖生成DAG，并从起点开始真正的计算。<br><img src="/blog-img/1219-3.png" alt=""><br>有了计算的DAG图，Spark内核下一步的任务就是根据DAG图将计算划分成任务集，也就是Stage，这样可以将任务提交到计算节点进行真正的计算。Spark计算的中间结果默认是保存在内存中的，Spark在划分Stage的时候会充分考虑在分布式计算中可流水线计算（pipeline）的部分来提高计算的效率，而在这个过程中，主要的根据就是RDD的依赖类型。根据不同的transformation操作，RDD的依赖可以分为窄依赖（Narrow Dependency）和宽依赖（Wide Dependency，在代码中为ShuffleDependency）两种类型。窄依赖指的是生成的RDD中每个partition只依赖于父RDD(s) 固定的partition。宽依赖指的是生成的RDD的每一个partition都依赖于父 RDD(s) 所有partition。窄依赖典型的操作有map, filter, union等，宽依赖典型的操作有groupByKey, sortByKey等。可以看到，宽依赖往往意味着shuffle操作，这也是Spark划分stage的主要边界。对于窄依赖，Spark会将其尽量划分在同一个stage中，因为它们可以进行流水线计算。<br><img src="/blog-img/1219-4.png" alt=""><br>我们再通过图4详细解释一下Spark中的Stage划分。我们从HDFS中读入数据生成3个不同的RDD，通过一系列transformation操作后再将计算结果保存回HDFS。可以看到这幅DAG中只有join操作是一个宽依赖，Spark内核会以此为边界将其前后划分成不同的Stage. 同时我们可以注意到，在图中Stage2中，从map到union都是窄依赖，这两步操作可以形成一个流水线操作，通过map操作生成的partition可以不用等待整个RDD计算结束，而是继续进行union操作，这样大大提高了计算的效率。<br><img src="/blog-img/1219-5.png" alt=""><br>Spark在运行时会把Stage包装成任务提交，有父Stage的Spark会先提交父Stage。弄清楚了Spark划分计算的原理，我们再结合源码看一看这其中的过程。下面的代码是DAGScheduler中的得到一个RDD父Stage的函数，可以看到宽依赖为划分Stage的边界。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">   * Get or create the list of parent stages for a given RDD. The stages will be assigned the</div><div class="line">   * provided jobId if they haven't already been created with a lower jobId.</div><div class="line">   */</div><div class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getParentStages</span></span>(rdd: <span class="type">RDD</span>[_], jobId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</div><div class="line">    <span class="keyword">val</span> parents = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">Stage</span>]</div><div class="line">    <span class="keyword">val</span> visited = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">RDD</span>[_]]</div><div class="line">    <span class="comment">// We are manually maintaining a stack here to prevent StackOverflowError</span></div><div class="line">    <span class="comment">// caused by recursively visiting</span></div><div class="line">    <span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">Stack</span>[<span class="type">RDD</span>[_]]</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">visit</span></span>(r: <span class="type">RDD</span>[_]) &#123;</div><div class="line">      <span class="keyword">if</span> (!visited(r)) &#123;</div><div class="line">        visited += r</div><div class="line">        <span class="comment">// Kind of ugly: need to register RDDs with the cache here since</span></div><div class="line">        <span class="comment">// we can't do it in its constructor because # of partitions is unknown</span></div><div class="line">        <span class="keyword">for</span> (dep &lt;- r.dependencies) &#123;</div><div class="line">          dep <span class="keyword">match</span> &#123;</div><div class="line">            <span class="keyword">case</span> shufDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt;</div><div class="line">              parents += getShuffleMapStage(shufDep, jobId)</div><div class="line">            <span class="keyword">case</span> _ =&gt;</div><div class="line">              waitingForVisit.push(dep.rdd)</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    waitingForVisit.push(rdd)</div><div class="line">    <span class="keyword">while</span> (!waitingForVisit.isEmpty) &#123;</div><div class="line">      visit(waitingForVisit.pop())</div><div class="line">    &#125;</div><div class="line">    parents.toList</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>上面提到Spark的计算是从RDD调用action操作时候触发的，我们来看一个action的代码:<br>RDD的collect方法是一个action操作，作用是将RDD中的数据返回到一个数组中。可以看到，在此action中，会触发Spark上下文环境SparkContext中的runJob方法，这是一系列计算的起点。<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">RDD</span>[<span class="type">T</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></div><div class="line">    @transient private var sc: <span class="type">SparkContext</span>,</div><div class="line">    @transient private var deps: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]]</div><div class="line">  ) <span class="keyword">extends</span> <span class="title">Serializable</span> <span class="keyword">with</span> <span class="title">Logging</span> &#123;</div><div class="line">  <span class="comment">//…</span></div><div class="line"><span class="comment">/**</span></div><div class="line">   * Return an array that contains all of the elements in this RDD.</div><div class="line">   */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">collect</span></span>(): <span class="type">Array</span>[<span class="type">T</span>] = &#123;</div><div class="line">    <span class="keyword">val</span> results = sc.runJob(<span class="keyword">this</span>, (iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; iter.toArray)</div><div class="line">    <span class="type">Array</span>.concat(results: _*)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>SparkContext拥有DAGScheduler的实例，在runJob方法中会进一步调用DAGScheduler的runJob方法。在此时，DAGScheduler会生成DAG和Stage，将Stage提交给TaskScheduler。TaskSchduler将Stage包装成TaskSet，发送到Worker节点进行真正的计算，同时还要监测任务状态，重试失败和长时间无返回的任务。整个过程如图5所示。<br><img src="/blog-img/1219-6.png" alt=""></p>
<p><strong>2.3 RDD的缓存与容错</strong></p>
<p>上文提到，Spark的计算是从action开始触发的，如果在action操作之前逻辑上很多transformation操作，一旦中间发生计算失败，Spark会重新提交任务，这在很多场景中代价过大。还有一些场景，如有些迭代算法，计算的中间结果会被重复使用，重复计算同样增加计算时间和造成资源浪费。因此，在提高计算效率和更好支持容错，Spark提供了基于RDD的cache机制和checkpoint机制。</p>
<p>我们可以通过RDD的toDebugString来查看其递归的依赖信息，图6展示了在spark shell中通过调用这个函数来查看wordCount RDD的依赖关系，也就是它的Lineage.<br><img src="/blog-img/1219-7.png" alt=""><br>如果发现Lineage过长或者里面有被多次重复使用的RDD，我们就可以考虑使用cache机制或checkpoint机制了。</p>
<p>我们可以通过在程序中直接调用RDD的cache方法将其保存在内存中，这样这个RDD就可以被多个任务共享，避免重复计算。另外，RDD还提供了更为灵活的persist方法，可以指定存储级别。从源码中可以看到RDD.cache就是简单的调用了RDD.persist(StorageLevel.MEMORY_ONLY)。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = persist()</div></pre></td></tr></table></figure>
<p>同样，我们可以调用RDD的checkpoint方法将其保存到磁盘。我们需要在SparkContext中设置checkpoint的目录，否则调用会抛出异常。值得注意的是，在调用checkpoint之前建议先调用cache方法将RDD放入内存，否则将RDD保存到文件的时候需要重新计算。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">   * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint</div><div class="line">   * directory set with SparkContext.setCheckpointDir() and all references to its parent</div><div class="line">   * RDDs will be removed. This function must be called before any job has been</div><div class="line">   * executed on this RDD. It is strongly recommended that this RDD is persisted in</div><div class="line">   * memory, otherwise saving it on a file will require recomputation.</div><div class="line">   */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">checkpoint</span></span>() &#123;</div><div class="line">    <span class="keyword">if</span> (context.checkpointDir.isEmpty) &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Checkpoint directory has not been set in the SparkContext"</span>)</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (checkpointData.isEmpty) &#123;</div><div class="line">      checkpointData = <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">RDDCheckpointData</span>(<span class="keyword">this</span>))</div><div class="line">      checkpointData.get.markForCheckpoint()</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<p>Cache机制和checkpoint机制的差别在于cache将RDD保存到内存，并保留Lineage，如果缓存失效RDD还可以通过Lineage重建。而checkpoint将RDD落地到磁盘并切断Lineage，由文件系统保证其重建。</p>
<p><strong>2.4 Spark任务的部署</strong></p>
<p>Spark的集群部署分为Standalone、Mesos和Yarn三种模式，我们以Standalone模式为例，简单介绍Spark程序的部署。如图7示，集群中的Spark程序运行时分为3种角色，driver, master和worker（slave）。在集群启动前，首先要配置master和worker节点。启动集群后，worker节点会向master节点注册自己，master节点会维护worker节点的心跳。Spark程序都需要先创建Spark上下文环境，也就是SparkContext。创建SparkContext的进程就成为了driver角色，上一节提到的DAGScheduler和TaskScheduler都在driver中运行。Spark程序在提交时要指定master的地址，这样可以在程序启动时向master申请worker的计算资源。Driver，master和worker之间的通信由Akka[4]支持。Akka 也使用 Scala 编写，用于构建可容错的、高可伸缩性的Actor 模型应用。关于Akka，可以访问其官方网站进行进一步了解，本文不做详细介绍。</p>
<p><img src="/blog-img/1219-8.png" alt=""></p>
<p><strong>3. 更深一步了解Spark内核</strong></p>
<p>了解了Spark内核的基本概念和实现后，更深一步理解其工作原理的最好方法就是阅读源码。最新的Spark源码可以从Spark官方[3]网站下载。源码推荐使用IntelliJ IDEA阅读，会自动安装Scala插件。读者可以从core工程，也就是Spark内核工程开始阅读，更可以设置断点尝试跟踪一个任务的执行。另外，读者还可以通过分析Spark的日志来进一步理解Spark的运行机制，Spark使用log4j记录日志，可以在启动集群前修改log4j的配置文件来配置日志输出和格式。</p>
<p><strong>参考资料</strong><br>[1] <a href="http://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html" target="_blank" rel="external">http://spark.apache.org/news/spark-wins-daytona-gray-sort-100tb-benchmark.html</a><br>[2] <a href="https://amplab.cs.berkeley.edu/software/" target="_blank" rel="external">https://amplab.cs.berkeley.edu/software/</a><br>[3] <a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">http://spark.apache.org/downloads.html</a><br>[4] <a href="http://akka.io/docs/" target="_blank" rel="external">http://akka.io/docs/</a><br>[5] <a href="http://www.tachyonproject.org/" target="_blank" rel="external">http://www.tachyonproject.org/</a></p>
<p>本文来自： <a href="http://www.chinahadoop.cn/group/3/thread/956" target="_blank" rel="external">http://www.chinahadoop.cn/group/3/thread/956</a> 对原文有少许修改</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/**</span></div><div class="line">   * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint</div><div class="line">   * directory set with SparkContext.setCheckpointDir() and all references to its parent</div><div class="line">   * RDDs will be removed. This function must be called before any job has been</div><div class="line">   * executed on this RDD. It is strongly recommended that this RDD is persisted in</div><div class="line">   * memory, otherwise saving it on a file will require recomputation.</div><div class="line">   */</div><div class="line">  <span class="function"><span class="keyword">def</span> <span class="title">checkpoint</span></span>() &#123;</div><div class="line">    <span class="keyword">if</span> (context.checkpointDir.isEmpty) &#123;</div><div class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Checkpoint directory has not been set in the SparkContext"</span>)</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (checkpointData.isEmpty) &#123;</div><div class="line">      checkpointData = <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">RDDCheckpointData</span>(<span class="keyword">this</span>))</div><div class="line">      checkpointData.get.markForCheckpoint()</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure>
<h2 id="spark集群搭建"><a href="#spark集群搭建" class="headerlink" title="spark集群搭建"></a>spark集群搭建</h2><p>生产搭建</p>
<h2 id="spark-ui监控"><a href="#spark-ui监控" class="headerlink" title="spark-ui监控"></a>spark-ui监控</h2><h2 id="spark-streaming"><a href="#spark-streaming" class="headerlink" title="spark-streaming"></a>spark-streaming</h2>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[Python入门到精通]]></title>
      <url>https://jamesxu9093.github.io/2016/12/14/Python%E5%85%A5%E9%97%A8%E5%88%B0%E7%B2%BE%E9%80%9A%E4%B9%8B%E8%B7%AF/</url>
      <content type="html"><![CDATA[<p>一门语言的学习从来都不是一蹴而就的，需要对语言的方方面面进行学习，本文是自己打算要学的方向。包括如下方面：<br>1、Python语言基础<br>2、函数式编程与面向对象<br>3、消息队列<br>4、数据结构和算法<br>5、爬虫<br>6、web开发：flask，django，tornado<br>7、科学计算<br>8、机器学习<br><a id="more"></a></p>
<h2 id="Python语言基础"><a href="#Python语言基础" class="headerlink" title="Python语言基础"></a>Python语言基础</h2><p>廖雪峰老师的官网：<a href="http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000" target="_blank" rel="external">http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000</a><br>菜鸟网：<a href="http://www.runoob.com/python/python-tutorial.html" target="_blank" rel="external">http://www.runoob.com/python/python-tutorial.html</a><br>伯乐在线：<a href="http://www.jobbole.com/" target="_blank" rel="external">http://www.jobbole.com/</a><br>易伯教程：<a href="http://www.yiibai.com/" target="_blank" rel="external">http://www.yiibai.com/</a><br>自强编程网：<a href="http://www.ziqiangxuetang.com" target="_blank" rel="external">http://www.ziqiangxuetang.com</a></p>
<p>参考书籍：可以随时翻阅<br>python核心编程(第二版)<br>python技术手册(第二版)<br>Python学习手册(第4版)[中]<br><strong>The Python Standard Library by Example[中][英]</strong></p>
<h2 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h2><p><a href="http://python-rq.org/" target="_blank" rel="external">http://python-rq.org/</a></p>
<h2 id="爬虫"><a href="#爬虫" class="headerlink" title="爬虫"></a>爬虫</h2><p>用Python写网络爬虫.pdf(157页)<br>Python网络数据采集.pdf</p>
<h2 id="web开发"><a href="#web开发" class="headerlink" title="web开发"></a>web开发</h2><p>flask：<a href="http://docs.jinkan.org/docs/flask/" target="_blank" rel="external">http://docs.jinkan.org/docs/flask/</a> 0.10版中文文档<br>django：<a href="http://python.usyiyi.cn/documents/django_182/index.html" target="_blank" rel="external">http://python.usyiyi.cn/documents/django_182/index.html</a><br>tornado：<a href="http://demo.pythoner.com/itt2zh/" target="_blank" rel="external">http://demo.pythoner.com/itt2zh/</a><br>各种在线手册：<a href="http://docs.pythontab.com/" target="_blank" rel="external">http://docs.pythontab.com/</a></p>
<h2 id="科学计算"><a href="#科学计算" class="headerlink" title="科学计算"></a>科学计算</h2><p><a href="http://litaotao.github.io/python-finance-quant-investing" target="_blank" rel="external">http://litaotao.github.io/python-finance-quant-investing</a></p>
<h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><p>集体智慧编程<br>机器学习实战<br>Python数据挖掘入门与实践[中]</p>
<p>看书之前最好先看看书评，除非非常前沿的书，当你达到那个程度的时候<br><a href="http://www.toutiao.com/i6364137837293470210/?tt_from=mobile_qq&amp;utm_campaign=client_share&amp;app=news_article&amp;utm_source=mobile_qq&amp;iid=6700376695&amp;utm_medium=toutiao_ios" target="_blank" rel="external">http://www.toutiao.com/i6364137837293470210/?tt_from=mobile_qq&amp;utm_campaign=client_share&amp;app=news_article&amp;utm_source=mobile_qq&amp;iid=6700376695&amp;utm_medium=toutiao_ios</a><br>介绍了两个书<br>Python Machine Learning（<a href="http://it.sohu.com/20161102/n472103874.shtml）" target="_blank" rel="external">http://it.sohu.com/20161102/n472103874.shtml）</a><br>Data Science from Scratch(First Principles with Python) （<a href="http://download.csdn.net/detail/ramissue/8875645" target="_blank" rel="external">http://download.csdn.net/detail/ramissue/8875645</a> 评论）</p>
<h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><p>Python网络编程攻略(170页)<br>Python网络编程基础(websocket，线程等)</p>
<h2 id="资源总结"><a href="#资源总结" class="headerlink" title="资源总结"></a>资源总结</h2><p>实验楼</p>
<p>packt的书：很多短小但是实用 <a href="https://www.packtpub.com/" target="_blank" rel="external">https://www.packtpub.com/</a><br>学好英语，可以在国外的网站上获取很多类似的书籍资源<br>图书下载：<a href="http://www.allitebooks.com" target="_blank" rel="external">http://www.allitebooks.com</a><br>oreilly书籍源码下载:<a href="https://ssearch.oreilly.com/?q=&amp;x=0&amp;y=0" target="_blank" rel="external">https://ssearch.oreilly.com/?q=&amp;x=0&amp;y=0</a><br>python、git等书籍的脑图：<a href="http://www.pythoner.com/" target="_blank" rel="external">http://www.pythoner.com/</a></p>
]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[DataStructuresAndAlgorithmsInPython读书笔记]]></title>
      <url>https://jamesxu9093.github.io/2016/12/09/DataStructuresAndAlgorithmsInPython/</url>
      <content type="html"><![CDATA[<p>DataStructuresAndAlgorithmsInPython是2013年出的一本python数据结构与算法的书，全书共15章，篇幅不短，<br>有750来页，豆瓣读书上 <a href="https://book.douban.com/review/6854650/" target="_blank" rel="external">https://book.douban.com/review/6854650/</a> 这篇文章总结的相当好！！！</p>
<p><img src="/blog-img/1209-2.png" alt=""></p>
<a id="more"></a>
<p><strong>Ch1 Python 基础知识</strong><br>这部分对Python的基本语法做了一个简单的描述，主要目的是让本书的读者在一个相同的起跑线上，方便后面的内容展开。</p>
<p><strong>Ch2 面向对象编程</strong><br>首先谈到了面向对象的设计模式，然后以一个例子讲解了Python中如何定义一个类，以及类与类之间的继承关系。最后介绍了Python中类内的变量管理。</p>
<p><strong>Ch3 算法分析基础</strong><br>这部分几乎是算法书必备的内容。介绍如何分析算法的复杂度。</p>
<p><strong>Ch4 递归</strong><br>在开始讲数据结构之前，作者先介绍了一下递归的思想，个人感觉这个章节的安排稍微有些唐突，不过，也算是为后面做铺垫吧。作者对于递归的分类很有启发意义。</p>
<p><strong>Ch5 基于Array的序列</strong><br>忘掉Python下常用的list等等数据结构吧，作者先从最最基础的ctypes下的array结构开始构造类似于list的动态序列数据类型。这将为大家理解list类型奠定良好基础。C语言基础很好的话理解起来会很快。</p>
<p><strong>Ch6 栈、队列与双向队列</strong><br>作者从ADT（Abstract Data Type）出发，在前面实现的基于Array的序列基础上，实现了栈、队列、双向队列这三种数据结构。</p>
<p><strong>Ch7 链表</strong><br>该部分将前面已经实现的三种数据结构糅合在一起，在介绍了链表后，通过链表来实现上一章提到的集中数据结构。</p>
<p><strong>Ch8 树</strong><br>从树，再到二叉树，然后深入其中，借用Array序列和链表来实现树这个类。最后介绍了遍历树的简单算法。</p>
<p><strong>Ch9 优先队列</strong><br>同样，这部分先是介绍了优先队列后，分别用有序列表和无序列表实现了优先队列。然后由此引出了堆。再根据优先队列中的排序问题分别分析了选择排序、插入排序以及堆排序。</p>
<p><strong>Ch10 Map、哈希表以及跳表</strong><br>这部分内容的重点是介绍了Hash的思想。此外作者跳出Python下常用的字典类型，对Map进行了不同的分类并实现。</p>
<p><strong>Ch11 搜索树</strong><br>这部分内容主要围绕平衡树展开，介绍了AVL、红黑树等等。</p>
<p><strong>Ch12 排序与选择</strong><br>尽管前面已经提到了集中排序算法，这里作者补充了并排、快排以及桶排序等等算法并做了比较。</p>
<p><strong>Ch13 文本处理</strong><br>该部分主要是字符串查找的优化，重点分析了动态问题编程的思想。该部分还介绍了trie树（字典树）</p>
<p><strong>Ch14 图</strong><br>该部分虽然简短，但覆盖面广，包含了图的结构、图的遍历、最短路径以及最小生成树等等。需要参考其他书作为补充。</p>
<p><strong>Ch15 内存管理与B树</strong><br>介绍了Python中内存管理体系（内存分配，垃圾回收，缓存机制等等），并介绍了B树。</p>
<h2 id="Python-基础知识"><a href="#Python-基础知识" class="headerlink" title="Python 基础知识"></a>Python 基础知识</h2><h3 id="python是一门动态类、解释型的语言"><a href="#python是一门动态类、解释型的语言" class="headerlink" title="python是一门动态类、解释型的语言"></a>python是一门动态类、解释型的语言</h3><h3 id="python是面向对象的语言，函数也是对象"><a href="#python是面向对象的语言，函数也是对象" class="headerlink" title="python是面向对象的语言，函数也是对象"></a>python是面向对象的语言，函数也是对象</h3><h3 id="表达式、操作符、优先级"><a href="#表达式、操作符、优先级" class="headerlink" title="表达式、操作符、优先级"></a>表达式、操作符、优先级</h3><h3 id="流程控制"><a href="#流程控制" class="headerlink" title="流程控制"></a>流程控制</h3><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><h3 id="简单输入输出、File"><a href="#简单输入输出、File" class="headerlink" title="简单输入输出、File"></a>简单输入输出、File</h3><h3 id="异常处理"><a href="#异常处理" class="headerlink" title="异常处理"></a>异常处理</h3><h3 id="迭代器、生成器"><a href="#迭代器、生成器" class="headerlink" title="迭代器、生成器"></a>迭代器、生成器</h3><h3 id="语言优势–额外的便利：条件表达式、推导式语法、序列自动装拆箱"><a href="#语言优势–额外的便利：条件表达式、推导式语法、序列自动装拆箱" class="headerlink" title="语言优势–额外的便利：条件表达式、推导式语法、序列自动装拆箱"></a>语言优势–额外的便利：条件表达式、推导式语法、序列自动装拆箱</h3><p>simultaneous assignment 同时赋值<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">x, y, z = <span class="number">6</span>, <span class="number">2</span>, <span class="number">5</span> </div><div class="line"><span class="comment"># 发生的行为是：等号右边先automatically packed 成一个typle,然后automatically unpacked 赋值给左边</span></div><div class="line">j, k = k, j</div><div class="line"><span class="comment"># 等效于：</span></div><div class="line">temp = j</div><div class="line">j = k</div><div class="line">k = temp</div></pre></td></tr></table></figure></p>
<h3 id="作用域和命名空间"><a href="#作用域和命名空间" class="headerlink" title="作用域和命名空间"></a>作用域和命名空间</h3><p>检查命名空间的标识符(字典的key):dir()<br>检查命名空间的标识符(full dict):vars()</p>
<h3 id="模块化编程"><a href="#模块化编程" class="headerlink" title="模块化编程"></a>模块化编程</h3><p>Modules are also first-class objects in Python<br>伪随机数<br>Mersenne twister 梅森旋转算法</p>
<h2 id="面向对象编程"><a href="#面向对象编程" class="headerlink" title="面向对象编程"></a>面向对象编程</h2><h3 id="目标，原则，模式"><a href="#目标，原则，模式" class="headerlink" title="目标，原则，模式"></a>目标，原则，模式</h3><p>目标<br>原则<br>设计模式</p>
<h3 id="软件开发"><a href="#软件开发" class="headerlink" title="软件开发"></a>软件开发</h3><p>设计<br>伪代码<br>编码风格、文档注释<br>测试</p>
<h3 id="类定义"><a href="#类定义" class="headerlink" title="类定义"></a>类定义</h3><p>操作符重载</p>
<h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><h3 id="命名空间和面向对象"><a href="#命名空间和面向对象" class="headerlink" title="命名空间和面向对象"></a>命名空间和面向对象</h3><h3 id="浅拷贝和深拷贝"><a href="#浅拷贝和深拷贝" class="headerlink" title="浅拷贝和深拷贝"></a>浅拷贝和深拷贝</h3>]]></content>
    </entry>
    
    <entry>
      <title><![CDATA[hexo + github page构建一个自用简易博客平台，使用markdown语法写作]]></title>
      <url>https://jamesxu9093.github.io/2016/12/08/markdown/</url>
      <content type="html"><![CDATA[<p>使用hexo + github page 构建一个自用博客平台，的一般步骤(以windows为例)：<br>1.安装git客户端<br>2.安装node.js<br>3.安装hexo，配置<br>4.下载主题，配置<br>5.学习markdown语法并进行写作</p>
<p>参考网站 <a href="https://fangyeqing.github.io/2016/10/28/hexo+github_page%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/" target="_blank" rel="external">https://fangyeqing.github.io/2016/10/28/hexo+github_page%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/</a><br><a id="more"></a></p>
<h1 id="Headers"><a href="#Headers" class="headerlink" title="Headers"></a>Headers</h1><h2 id="This-is-an-h2-tag"><a href="#This-is-an-h2-tag" class="headerlink" title="This is an h2 tag"></a>This is an h2 tag</h2><h2 id="This-is-an-h2-tag-1"><a href="#This-is-an-h2-tag-1" class="headerlink" title="This is an h2 tag"></a>This is an h2 tag</h2><h3 id="This-is-an-h3-tag"><a href="#This-is-an-h3-tag" class="headerlink" title="This is an h3 tag"></a>This is an h3 tag</h3><h3 id="This-is-an-h3-tag-1"><a href="#This-is-an-h3-tag-1" class="headerlink" title="This is an h3 tag"></a>This is an h3 tag</h3><h4 id="This-is-an-h4-tag"><a href="#This-is-an-h4-tag" class="headerlink" title="This is an h4 tag"></a>This is an h4 tag</h4><h1 id="Emphasis"><a href="#Emphasis" class="headerlink" title="Emphasis"></a>Emphasis</h1><p><em>This text will be italic</em><br><em>This will also be italic</em></p>
<p><strong>This text will be bold</strong><br><strong>This will also be bold</strong></p>
<p><em>You <strong>can</strong> combine them</em></p>
<h1 id="Lists"><a href="#Lists" class="headerlink" title="Lists"></a>Lists</h1><h2 id="Unordered"><a href="#Unordered" class="headerlink" title="Unordered"></a>Unordered</h2><ul>
<li>Item 1</li>
<li>Item 2<ul>
<li>Item 2a</li>
<li>Item 2b</li>
</ul>
</li>
</ul>
<h2 id="Ordered"><a href="#Ordered" class="headerlink" title="Ordered"></a>Ordered</h2><ol>
<li>Item 1</li>
<li>Item 2</li>
<li>Item 3<ul>
<li>Item 3a</li>
<li>Item 3b</li>
</ul>
</li>
</ol>
<h1 id="Images"><a href="#Images" class="headerlink" title="Images"></a>Images</h1><p><img src="/images/logo.png" alt="GitHub Logo"><br>Format: <img src="url" alt="Alt Text"><br><img src="/img/littleboy.png" alt="littleboy Logo"><br><img src="/blog-img/1209-1.png" alt=""></p>
<h1 id="Links"><a href="#Links" class="headerlink" title="Links"></a>Links</h1><p><a href="http://github.com" target="_blank" rel="external">http://github.com</a> - automatic!<br><a href="http://github.com" target="_blank" rel="external">GitHub</a></p>
<h1 id="Blockquotes"><a href="#Blockquotes" class="headerlink" title="Blockquotes"></a>Blockquotes</h1><p>As Kanye West said:</p>
<blockquote>
<p>We’re living the future so<br>the present is our past.</p>
</blockquote>
<h1 id="Inline-code"><a href="#Inline-code" class="headerlink" title="Inline code"></a>Inline code</h1><p>I think you should use an<br><code>&lt;addr&gt;</code> element here instead.</p>
<h1 id="programming-language"><a href="#programming-language" class="headerlink" title="programming language"></a>programming language</h1><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">function</span> <span class="title">fancyAlert</span>(<span class="params">arg</span>) </span>&#123;</div><div class="line">  <span class="keyword">if</span>(arg) &#123;</div><div class="line">    $.facebox(&#123;<span class="attr">div</span>:<span class="string">'#foo'</span>&#125;)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> bar:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">True</span></div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<h1 id="Task-Lists"><a href="#Task-Lists" class="headerlink" title="Task Lists"></a>Task Lists</h1><ul>
<li>[x] @mentions, #refs, <a href="">links</a>, <strong>formatting</strong>, and <del>tags</del> supported</li>
<li>[x] list syntax required (any unordered or ordered list supported)</li>
<li>[x] this is a complete item</li>
<li>[ ] this is an incomplete item</li>
</ul>
<p>If you include a task list in the first comment of an Issue, you will get a handy progress indicator in your issue list. It also works in Pull Requests!</p>
<h1 id="Tables"><a href="#Tables" class="headerlink" title="Tables"></a>Tables</h1><table>
<thead>
<tr>
<th>First Header</th>
<th>Second Header</th>
</tr>
</thead>
<tbody>
<tr>
<td>Content from cell 1</td>
<td>Content from cell 2</td>
</tr>
<tr>
<td>Content in the first column</td>
<td>Content in the second column</td>
</tr>
</tbody>
</table>
<font face="黑体">我是黑体字</font><br><font face="微软雅黑">我是微软雅黑</font><br><font face="STCAIYUN">我是华文彩云</font><br><font color="#0099ff" size="7" face="黑体">color=#0099ff size=72 face=”黑体”</font><br><font color="#00ffff" size="72">color=#00ffff</font><br><font color="gray" size="72">color=gray</font>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">可以当作块来看</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">---</div><div class="line">title: Hello World</div><div class="line">date: 2015-12-03 00:00:00</div><div class="line">categories: samza</div><div class="line">tags: [samza,学习,hello world] </div><div class="line">toc: true</div><div class="line">---</div><div class="line">&lt;Excerpt in index | 首页摘要&gt; </div><div class="line">摘要内容...</div><div class="line">&lt;!-- more --&gt;</div><div class="line">&lt;The rest of contents | 余下全文&gt;</div><div class="line">余下的全文内容...</div></pre></td></tr></table></figure>]]></content>
    </entry>
    
  
  
</search>
